
% -------------------------------------------------------------------------------------
\begin{frame}[fragile]{Datasets Details (1)}

  \scriptsize
  \begin{table}[ht]
\centering
\caption{\small Overview of the datasets used in the experiments, showing for each table of each dataset the number of rows, the number of features (feature columns) and tables referred by foreign keys (the parent tables).}

\begin{tabular}{l l r r l}
\toprule
\textbf{Dataset} & \textbf{Table} & \textbf{\# Rows} & \textbf{\# Features} & \textbf{Foreign Keys} \\
\cmidrule(lr){1-5}
\multirow{2}{*}{AirBnB} 
  & users    & $10,000$  & $15$ & -- \\
  & sessions & $47,217$  & $5$  & users \\
\addlinespace
\cmidrule(lr){1-5}
\multirow{5}{*}{Biodegradability} 
  & molecule & $328$     & $3$  & -- \\
  & group    & $1,736$   & $1$  & -- \\
  & atom     & $6,568$   & $1$  & molecule \\
  & gmember  & $6,647$   & --  & atom, group \\
  & bond     & $6,616$   & $1$  & atom1, atom2 \\
\addlinespace
\cmidrule(lr){1-5}
\multirow{3}{*}{CORA} 
  & paper    & $2,708$   & $1$  & -- \\
  & content  & $49,216$  & $1$  & paper \\
  & cites    & $5,429$   & --  & paper1, paper2 \\
\bottomrule
\end{tabular}


\end{table}

\end{frame}
% -------------------------------------------------------------------------------------


% -------------------------------------------------------------------------------------
\begin{frame}[fragile]{Datasets Details (2)}

  \scriptsize
  \begin{table}[ht]
\centering
\caption{\small Overview of the datasets used in the experiments, showing for each table of each dataset the number of rows, the number of features (feature columns) and tables referred by foreign keys (the parent tables).}

\begin{tabular}{l l r r l}
\toprule
\textbf{Dataset} & \textbf{Table} & \textbf{\# Rows} & \textbf{\# Features} & \textbf{Foreign Keys} \\
\cmidrule(lr){1-5}
\multirow{7}{*}{IMDB MovieLens} 
  & users            & $6,039$    & $3$  & -- \\
  & movies           & $3,832$    & $4$  & -- \\
  & actors           & $98,690$   & $2$  & -- \\
  & directors        & $2,201$    & $2$  & -- \\
  & ratings          & $996,159$  & $1$  & movie, user \\
  & movies2actors    & $138,349$  & $1$  & movie, actor \\
  & movies2directors & $4,141$    & $1$  & movie, director \\
\addlinespace
\cmidrule(lr){1-5}
\multirow{2}{*}{Rossmann} 
  & store      & $1,115$   & $9$  & -- \\
  & historical & $57,970$  & $7$  & store \\
\addlinespace
\cmidrule(lr){1-5}
\multirow{3}{*}{Walmart} 
  & stores    & $45$     & $2$  & -- \\
  & features  & $225$    & $11$ & store \\
  & depts     & $15,047$ & $4$  & store \\
\bottomrule
\end{tabular}


\end{table}

\end{frame}
% -------------------------------------------------------------------------------------


% -------------------------------------------------------------------------------------
\begin{frame}[fragile]{GNN Architecture Details}

\textbf{Core Approach:}
\begin{itemize}
  \item GNNs compute node embeddings $\boldsymbol{\varepsilon}^i$ for each record $i$
  \item Handles heterogeneous graphs with multiple node/edge types
  \item Edge-type-specific convolutions aggregate neighbors by table type
\end{itemize}

\textbf{Two Architecture Variants:}
\begin{columns}[T,onlytextwidth]
  \column{0.48\textwidth}
  \textit{GATv2-based:} \citet{brody2021attentive}
  \begin{itemize}
    \item Attention mechanism
    \item Residual connections
    \item Hidden dim: 100
    \item Used for: AirBnB, Rossmann, Walmart
  \end{itemize}
  
  \column{0.48\textwidth}
  \textit{GIN-based:} \citet{xu2018powerful}
  \begin{itemize}
    \item 3 GIN layers
    \item MLP width: 100
    \item Embedding size: 20 or 50
    \item Used for: CORA, IMDB, Biodegradability
  \end{itemize}
\end{columns}

\end{frame}
% -------------------------------------------------------------------------------------

% -------------------------------------------------------------------------------------
\begin{frame}[fragile]{Table-specific Denoisers}

\textbf{Architecture:}
\begin{itemize}
  \item Each table $k$ has a dedicated MLP denoiser $f^k$ that predicts clean records:
  \begin{equation*}
  \hat{\mathbf{x}}^i_1 = f^{k_i}(\mathbf{x}^i_t, t, \boldsymbol{\varepsilon}^i_t)
  \end{equation*}
  \item \textbf{Inputs}: noisy record $\mathbf{x}^i_t$ + time embedding $t$ + node embedding $\boldsymbol{\varepsilon}^i_t$
  \item \textbf{Design}: Layer Normalization + SiLU activations + linear output
  \item \textbf{Output}: Softmax for categoricals, linear for continuous features
\end{itemize}

\textbf{Hyperparameters:}
\begin{itemize}
  \item 2-3 hidden layers per table
  \item Width: 10-1000 hidden units (dataset-specific)
\end{itemize}

\end{frame}
% -------------------------------------------------------------------------------------

% -------------------------------------------------------------------------------------
\begin{frame}[fragile]{Tuning Node Embedding Size}

\textbf{Key Tradeoff:}
\begin{itemize}
  \item Too large → memorization and privacy leaks
  \item Too small → limited expressiveness
  \item Our choice: 2-10 dimensions (conservative for privacy)
\end{itemize}

\begin{figure}[ht]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/embedding_vs_val_IMDB_MovieLens.pdf}
        
        (a) IMDB MovieLens
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/images/embedding_vs_val_AirBnB.pdf}
        
        (b) Airbnb
    \end{minipage}
    
    \vspace{0.3em}
    Validation loss vs. GNN embedding size (0 = no GNN)
\end{figure}

%\textbf{Observation:} IMDB degrades with larger embeddings; Airbnb less sensitive (different GNN).

\end{frame}
% -------------------------------------------------------------------------------------

\begin{frame}[fragile]{Results: Privacy and Efficiency}
  \begin{itemize}
    \item \textbf{Privacy}: DCR (Distance to Closest Record) analysis does not highlight privacy leaks. Intuitively, we check if generated records are too similar to real records in the training set.
    \item \textbf{Efficiency}: Training and generation are fast. \footnote{on a single NVIDIA RTX A5000}
  \end{itemize}

\begin{table}[h]
\centering
%\caption{Maximum runtime across repetitions for each dataset during experimentation.}
\label{tab:experiment_duration}
\begin{tabular}{l r}
\toprule
\textbf{Dataset Name} & \textbf{Running Time} \\
\midrule
AirBnB                & $10$m $3$s \\
Biodegradability      & $1$m $6$s  \\
CORA                  & $3$m $10$s \\
IMDB MovieLens        & $14$m $25$s \\
Rossmann              & $2$m $57$s \\
Walmart               & $1$m $48$s \\
\bottomrule
\end{tabular}
\end{table}

\end{frame}
% -------------------------------------------------------------------------------------